"""Attention Mechanism for LSTM"Implements scaled dot-product attention for time series prediction.Focuses on relevant temporal patterns for price forecasting."""import torchimport torch.nn as nnimport torch.nn.functional as Fimport mathfrom typing import Tuple, Optionalclass AttentionLayer(nn.Module):        """Scaled dot-product attention mechanism for LSTM outputs."""    def __init__(self, hidden_size: int, num_heads: int = 8, dropout: float = 0.1):
            """Initialize attention layer."        Args:
                hidden_size: Size of hidden state (must be divisible by num_heads)                num_heads: Number of attention heads                dropout: Dropout rate                """                super(AttentionLayer, self).__init__()                assert hidden_size % num_heads == 0, "hidden_size must be divisible by num_heads"                self.hidden_size = hidden_size                self.num_heads = num_heads                self.head_dim = hidden_size // num_heads        # Linear projections for Q, K, V                self.query_projection = nn.Linear(hidden_size, hidden_size)                self.key_projection = nn.Linear(hidden_size, hidden_size)                self.value_projection = nn.Linear(hidden_size, hidden_size)        # Output projection                self.output_projection = nn.Linear(hidden_size, hidden_size)        # Dropout and layer norm                self.dropout = nn.Dropout(dropout)                self.layer_norm = nn.LayerNorm(hidden_size)        # Initialize weights                self._init_weights()    def _init_weights(self):
            """Initialize weights using Xavier uniform."""        for module in [self.query_projection, self.key_projection,
        self.value_projection, self.output_projection]:
                nn.init.xavier_uniform_(module.weight)                nn.init.zeros_(module.bias)    def forward(self,
        lstm_output: torch.Tensor,        mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:            """Forward pass through attention mechanism."        Args:
                lstm_output: LSTM outputs [batch_size, seq_len, hidden_size]                mask: Optional padding mask [batch_size, seq_len]                Returns:                    attended_output: Attention-weighted output [batch_size, seq_len, hidden_size]                    attention_weights: Attention scores [batch_size, num_heads, seq_len, seq_len]                    """                    batch_size, seq_len, _ = lstm_output.shape        # Compute Q, K, V projections                    queries = self.query_projection(lstm_output)                    keys = self.key_projection(lstm_output)                    values = self.value_projection(lstm_output)        # Reshape for multi-head attention        # [batch_size, seq_len, hidden_size] -> [batch_size, num_heads, seq_len, head_dim]                    queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)                    keys = keys.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)                    values = values.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)        # Compute attention scores                    attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)        # Apply mask if provided                    if mask is not None:        # Expand mask for multi-head attention
                    pass  # Auto-fixed: empty block                    pass  # Auto-fixed: empty block                    pass  # Auto-fixed: empty block                    pass  # Auto-fixed: empty block                    pass  # Auto-fixed: empty block                    mask = mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]                    attention_scores.masked_fill_(mask == 0, float('-inf'))        # Apply softmax to get attention weights                    attention_weights = F.softmax(attention_scores, dim=-1)                    attention_weights = self.dropout(attention_weights)        # Apply attention to values                    attended_values = torch.matmul(attention_weights, values)        # Reshape back to [batch_size, seq_len, hidden_size]                    attended_values = attended_values.transpose(1, 2).contiguous().view(                    batch_size, seq_len, self.hidden_size                    )        # Apply output projection                    output = self.output_projection(attended_values)        # Residual connection and layer norm                    output = self.layer_norm(output + lstm_output)                    return output, attention_weightsclass TemporalAttention(nn.Module):    \"\"\"Temporal-focused attention for financial time series.        Emphasizes recent patterns while maintaining long-term context.    \"\"\"        def __init__(self, hidden_size: int, temporal_window: int = 20):        \"\"\"Initialize temporal attention.                Args:            hidden_size: Size of hidden state            temporal_window: Window for temporal decay        \"\"\"        super(TemporalAttention, self).__init__()                self.hidden_size = hidden_size        self.temporal_window = temporal_window                # Temporal decay weights        self.temporal_weights = nn.Parameter(            torch.exp(-torch.arange(temporal_window, dtype=torch.float) / temporal_window)        )                # Context vector for temporal attention        self.context_vector = nn.Parameter(torch.randn(hidden_size))                # Projection layers        self.attention_projection = nn.Linear(hidden_size, hidden_size)        self.output_projection = nn.Linear(hidden_size, hidden_size)            def forward(self, lstm_output: torch.Tensor) -> torch.Tensor:        \"\"\"Apply temporal attention to LSTM output.                Args:            lstm_output: LSTM outputs [batch_size, seq_len, hidden_size]                    Returns:            attended_output: Temporally weighted output [batch_size, hidden_size]        \"\"\"        batch_size, seq_len, _ = lstm_output.shape                # Compute attention scores using context vector        attention_input = torch.tanh(self.attention_projection(lstm_output))        attention_scores = torch.matmul(attention_input, self.context_vector)                # Apply temporal decay weights (focus on recent data)        if seq_len <= self.temporal_window:            temporal_weights = self.temporal_weights[-seq_len:]        else:            # Pad with small weights for older data            old_weights = torch.full((seq_len - self.temporal_window,), 0.01,                                    device=self.temporal_weights.device)            temporal_weights = torch.cat([old_weights, self.temporal_weights])                # Combine attention scores with temporal weights        combined_scores = attention_scores * temporal_weights.unsqueeze(0)        attention_weights = F.softmax(combined_scores, dim=1)                # Apply attention to get weighted representation        attended_output = torch.sum(lstm_output * attention_weights.unsqueeze(-1), dim=1)                # Final projection        output = self.output_projection(attended_output)                return outputclass FinancialAttention(nn.Module):    \"\"\"Financial market-specific attention mechanism.        Incorporates volatility and volume weighting for market-aware attention.    \"\"\"        def __init__(self, hidden_size: int, market_features: int = 5):        \"\"\"Initialize financial attention.                Args:            hidden_size: Size of hidden state            market_features: Number of market features (OHLCV)        \"\"\"        super(FinancialAttention, self).__init__()                self.hidden_size = hidden_size        self.market_features = market_features                # Market-aware attention weights        self.volatility_weight = nn.Linear(1, hidden_size)        self.volume_weight = nn.Linear(1, hidden_size)                # Attention mechanism        self.attention_layer = AttentionLayer(hidden_size, num_heads=4)                # Market feature projection        self.market_projection = nn.Linear(market_features, hidden_size)            def forward(self,                 lstm_output: torch.Tensor,                 market_data: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:        \"\"\"Apply financial market-aware attention.                Args:            lstm_output: LSTM outputs [batch_size, seq_len, hidden_size]            market_data: Market features [batch_size, seq_len, market_features]                    Returns:            attended_output: Market-weighted attention output            attention_weights: Attention scores        \"\"\"        batch_size, seq_len, _ = lstm_output.shape                # Extract volatility and volume from market data        # Assuming market_data contains [open, high, low, close, volume]        high_prices = market_data[:, :, 1:2]  # High prices        low_prices = market_data[:, :, 2:3]   # Low prices        volumes = market_data[:, :, 4:5]      # Volumes                # Calculate volatility (high-low range)        volatility = (high_prices - low_prices) / ((high_prices + low_prices) / 2 + 1e-8)                # Normalize volume (log transformation)        normalized_volume = torch.log(volumes + 1e-8)                # Generate market-aware weights        vol_weights = torch.sigmoid(self.volatility_weight(volatility))        volume_weights = torch.sigmoid(self.volume_weight(normalized_volume))                # Combine with LSTM output        market_weighted_output = lstm_output * (vol_weights + volume_weights) / 2                # Apply standard attention        attended_output, attention_weights = self.attention_layer(market_weighted_output)                return attended_output, attention_weights""