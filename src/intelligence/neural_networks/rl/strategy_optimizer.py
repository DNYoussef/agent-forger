"""Strategy Optimizer RL Agent"Main interface for reinforcement learning-based trading strategy optimization.Integrates PPO and A3C agents with GaryTaleb principles."""import torchimport numpy as npimport pandas as pdfrom typing import Dict, List, Tuple, Optional, Any, Unionfrom dataclasses import dataclassimport timefrom pathlib import Pathfrom .ppo_agent import PPOAgent, PPOConfig, create_ppo_agentfrom .trading_environment import TradingEnvironment, EnvironmentConfig, TradingState@dataclassclass StrategyConfig:        """Configuration for strategy optimizer."""    # RL Algorithm choice        algorithm: str = 'ppo'  # 'ppo' or 'a3c'    # Environment configuration        initial_capital: float = 200.0        max_position_size: float = 1.0        transaction_cost: float = 0.001        lookback_window: int = 60    # GaryxTaleb integration        dpi_reward_weight: float = 0.3        antifragile_reward_weight: float = 0.2        volatility_opportunity_weight: float = 0.15    # Training parameters        episodes_per_update: int = 10        max_episodes: int = 1000        eval_frequency: int = 50        save_frequency: int = 100    # Performance targets        target_return: float = 0.20  # 20% annual return        max_drawdown_limit: float = 0.10  # 10% maximum drawdown        min_sharpe_ratio: float = 1.5        target_inference_ms: float = 50.0class StrategyOptimizerRL:        """Main RL-based trading strategy optimizer."        Features:            - Multi-algorithm support (PPO, A3C)            - Gary's DPI integration'            - Taleb's antifragility principles'            - Real-time strategy adaptation            - Performance monitoring and optimization            """    def __init__(self,
        market_data: pd.DataFrame,        config: StrategyConfig):            """Initialize strategy optimizer."        Args:
                market_data: Historical market data for training/testing                config: Strategy optimizer configuration                """                self.config = config                self.market_data = market_data.copy()# Create trading environment                env_config = EnvironmentConfig(                initial_capital=config.initial_capital,                max_position_size=config.max_position_size,                transaction_cost=config.transaction_cost,                lookback_window=config.lookback_window,                dpi_reward_weight=config.dpi_reward_weight,                antifragile_reward_weight=config.antifragile_reward_weight,                volatility_opportunity_weight=config.volatility_opportunity_weight                )                self.env = TradingEnvironment(market_data, env_config)# Create RL agent based on algorithm choice                if config.algorithm.lower() == 'ppo':                    self.agent = self._create_ppo_agent()                else:                        raise ValueError(f"Algorithm {config.algorithm} not implemented yet"}# Performance tracking                        self.training_history = {                        'episodes': [],                        'returns': [],                        'sharpe_ratios': [],                        'max_drawdowns': [],                        'win_rates': [],                        'gary_dpi_contributions': [],                        'taleb_antifragile_contributions'} []                        }# Real-time adaptation parameters                        self.adaptation_window = 100  # Episodes for adaptation                        self.performance_threshold = 0.1  # Trigger for adaptation# GaryxTaleb integration state                        self.gary_dpi_state = {                        'momentum_signals': [],                        'volume_confirmations': [],                        'technical_alignments': []                        }                        self.taleb_antifragile_state = {                        'volatility_opportunities': [],                        'asymmetric_payoffs': [],                        'convexity_captures': []                        }    def _create_ppo_agent(self) -> PPOAgent:
            """Create PPO agent with optimized configuration."""        return create_ppo_agent(
        observation_shape=self.env.observation_space.shape,
        action_dim=self.env.action_space.shape[0],
        learning_rate=3e-4,
        enable_dpi=self.config.dpi_reward_weight > 0,
        enable_antifragile=self.config.antifragile_reward_weight > 0,
        fast_inference=True
        )
    def train(self,
        num_episodes: Optional[int] = None,        verbose: bool = True) -> Dict[str, Any]:            """Train the RL agent on market data."        Args:
                num_episodes: Number of training episodes (defaults to config)                verbose: Whether to print training progress                Returns:                    Training results and statistics                    """                    if num_episodes is None:                        num_episodes = self.config.max_episodes                        training_start = time.time()                        for episode in range(num_episodes):                            episode_start = time.time()# Reset environment                            observation, info = self.env.reset()                            episode_reward = 0                            episode_steps = 0# Episode loop                            done = False                            truncated = False                            while not done and not truncated:                # Get current market volatility for volatility adaptation                            pass  # Auto-fixed: empty block                            pass  # Auto-fixed: empty block                            pass  # Auto-fixed: empty block                            pass  # Auto-fixed: empty block                            pass  # Auto-fixed: empty block                            current_volatility = info['state'].volatility if 'state' in info else None# Get action from agent                            action, action_info = self.agent.get_action(                            observation,                            volatility=current_volatility,                            deterministic=False                            )# Take step in environment                            next_observation, reward, done, truncated, info = self.env.step(action)# Store transition                            self.agent.store_transition(                            observation=observation,                            action=action,                            reward=reward,                            value=action_info['value'],                            log_prob=action_info['log_prob'],                            done=done or truncated,                            volatility=current_volatility                            )# Update state                            observation = next_observation                            episode_reward += reward                            episode_steps += 1                                # Track GaryTaleb contributions                            if 'gary_dpi_contribution' in info:                                self.gary_dpi_state['momentum_signals'].append(                                info.get('gary_dpi_momentum', 0)                                )                                                    if 'taleb_antifragile_contribution' in info:                                    self.taleb_antifragile_state['volatility_opportunities'].append(                                    info.get('taleb_volatility_benefit', 0)                                    )                            # Episode finished - get metrics
        episode_metrics = self.env.get_episode_metrics()                    # Store episode results

        self.training_history['episodes'].append(episode)                                    self.training_history['returns'].append(episode_metrics.get('total_return', 0))                                    self.training_history['sharpe_ratios'].append(episode_metrics.get('sharpe_ratio', 0))                                    self.training_history['max_drawdowns'].append(episode_metrics.get('max_drawdown', 0))                                    self.training_history['win_rates'].append(episode_metrics.get('win_rate', 0))                                    self.training_history['gary_dpi_contributions'].append(                                    episode_metrics.get('gary_dpi_contribution', 0)                                    )                                    self.training_history['taleb_antifragile_contributions'].append(                                    episode_metrics.get('taleb_antifragile_contribution', 0)                                    )                    # Update agent if buffer is ready

        if episode % self.config.episodes_per_update == 0:                                        update_stats = self.agent.update()                                                        if verbose and update_stats:                                            print(f\"Episode {episode): Update completed\")                                            print(f\"  Policy Loss: {update_stats.get('policy_loss', 0):.4f)\")                                            print(f\"  Value Loss: {update_stats.get('value_loss', 0):.4f)\")                                            print(f\"  Learning Rate: {update_stats.get('learning_rate', 0):.6f)\")                            # Periodic evaluation

        if episode % self.config.eval_frequency == 0 and episode > 0:                                                eval_results = self.evaluate(num_episodes=5, verbose=False)                                                                if verbose:                                                    print(f\"\                                                    Episode {episode) Evaluation:\")                                                    print(f\"  Avg Return: {eval_results['avg_return']:.4f)\")                                                    print(f\"  Avg Sharpe: {eval_results['avg_sharpe_ratio']:.4f)\")                                                    print(f\"  Avg Max DD: {eval_results['avg_max_drawdown']:.4f)\")                                                    print(f\"  Gary DPI Score: {eval_results['avg_gary_dpi_score']:.4f)\")                                                    print(f\"  Taleb Antifragile Score: {eval_results['avg_antifragile_score']:.4f)\")                                    # Adaptive strategy adjustment                                                    self._adapt_strategy(eval_results)                        # Save model periodically

        if episode % self.config.save_frequency == 0 and episode > 0:                                                        self.save_model(f\"model_episode_{episode).pt\")                        # Progress logging

        if verbose and episode % 10 == 0:                                                            episode_time = time.time() - episode_start                                                            print(f\"Episode {episode)/{num_episodes) completed in {episode_time:.2f)s\")                                                            print(f\"  Episode Reward: {episode_reward:.4f)\")                                                            print(f\"  Episode Steps: {episode_steps)\")                                                            print(f\"  Total Return: {episode_metrics.get('total_return', 0):.4f)\")                                                                            training_time = time.time() - training_start                # Final training summary                                                            training_results = {                                                            'total_episodes': num_episodes,                                                            'training_time_minutes': training_time / 60,                                                            'avg_return': np.mean(self.training_history['returns'][-100:]),                                                            'avg_sharpe_ratio': np.mean(self.training_history['sharpe_ratios'][-100:]),                                                            'avg_max_drawdown': np.mean(self.training_history['max_drawdowns'][-100:]),                                                            'avg_win_rate': np.mean(self.training_history['win_rates'][-100:]),                                                            'gary_dpi_avg_contribution': np.mean(self.training_history['gary_dpi_contributions'][-100:]),                                                            'taleb_antifragile_avg_contribution': np.mean(self.training_history['taleb_antifragile_contributions'][-100:]),                                                            'performance_targets_met': self._check_performance_targets(),                                                            'agent_metrics': self.agent.get_performance_metrics()                                                            )                                                                    return training_results            def evaluate(self,

        num_episodes: int = 10,        deterministic: bool = True,        verbose: bool = True) -> Dict[str, Any]:        \"\"\"Evaluate trained agent performance.
                Args:
                num_episodes: Number of evaluation episodes                deterministic: Use deterministic policy                verbose: Print evaluation progress                            Returns:                    Evaluation results                    \"\"\"                    eval_returns = []                    eval_sharpe_ratios = []                    eval_max_drawdowns = []                    eval_win_rates = []                    eval_gary_dpi_scores = []                    eval_antifragile_scores = []                    eval_inference_times = []                            for episode in range(num_episodes):                        observation, info = self.env.reset()                        episode_return = 0                        episode_steps = 0                        inference_times = []                                    done = False                        truncated = False                                    while not done and not truncated:                            current_volatility = info['state'].volatility if 'state' in info else None                                # Get action (deterministic for evaluation)                            action, action_info = self.agent.get_action(                            observation,                            volatility=current_volatility,                            deterministic=deterministic                            )                                            inference_times.append(action_info['inference_time_ms'])                                # Take step                            observation, reward, done, truncated, info = self.env.step(action)                            episode_return += reward                            episode_steps += 1                        # Get episode metrics
        episode_metrics = self.env.get_episode_metrics()                                        eval_returns.append(episode_metrics.get('total_return', 0))                            eval_sharpe_ratios.append(episode_metrics.get('sharpe_ratio', 0))                            eval_max_drawdowns.append(episode_metrics.get('max_drawdown', 0))                            eval_win_rates.append(episode_metrics.get('win_rate', 0))                            eval_gary_dpi_scores.append(episode_metrics.get('gary_dpi_contribution', 0))                            eval_antifragile_scores.append(episode_metrics.get('taleb_antifragile_contribution', 0))                            eval_inference_times.extend(inference_times)                                        if verbose:                                print(f\"Eval Episode {episode+1): Return={episode_metrics.get('total_return', 0):.4f), \"                                f\"Sharpe={episode_metrics.get('sharpe_ratio', 0):.4f)\")                                                      eval_results = {                                'num_episodes': num_episodes,                                'avg_return': np.mean(eval_returns),                                'std_return': np.std(eval_returns),                                'avg_sharpe_ratio': np.mean(eval_sharpe_ratios),                                'avg_max_drawdown': np.mean(eval_max_drawdowns),                                'avg_win_rate': np.mean(eval_win_rates),                                'avg_gary_dpi_score': np.mean(eval_gary_dpi_scores),                                'avg_antifragile_score': np.mean(eval_antifragile_scores),                                'avg_inference_time_ms': np.mean(eval_inference_times),                                'max_inference_time_ms': np.max(eval_inference_times),                                'inference_target_met': np.mean(eval_inference_times) < self.config.target_inference_ms,                                'performance_consistency': 1 - (np.std(eval_returns) / (abs(np.mean(eval_returns)) + 1e-8)),                                'risk_adjusted_return': np.mean(eval_returns) / (np.mean(eval_max_drawdowns) + 1e-8)                                )                                        return eval_results            def optimize_single_action(self,

        observation: np.ndarray,        market_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:        \"\"\"Optimize single trading action for real-time use.
                Args:
                observation: Current market observation                market_context: Additional market context (volatility, etc.)                            Returns:                    Optimized action with context and timing                    \"\"\"                    start_time = time.time()                # Extract volatility from context                    volatility = market_context.get('volatility') if market_context else None                # Get action from agent                    action, action_info = self.agent.get_action(                    observation,                    volatility=volatility,                    deterministic=True  # Use deterministic policy for production                    )                # Gary's DPI analysis'                    dpi_analysis = self._analyze_gary_dpi_factors(observation, market_context)                # Taleb's antifragility assessment'                    antifragile_assessment = self._assess_taleb_antifragility(observation, market_context)                # Combine insights                    optimization_time = (time.time() - start_time) * 1000                            result = {                    'action': action,                    'position_change': float(action[0]),  # Position change [-1, 1]                    'confidence': float(action[1]),       # Confidence [0, 1]                    'value_estimate': action_info['value'],                    'gary_dpi_analysis': dpi_analysis,                    'taleb_antifragile_assessment': antifragile_assessment,                    'optimization_time_ms': optimization_time,                    'total_inference_time_ms': action_info['inference_time_ms'] + optimization_time,                    'real_time_ready': (action_info['inference_time_ms'] + optimization_time) < self.config.target_inference_ms,                    'market_context_used': market_context is not None,                    'volatility_adapted': volatility is not None                    )                            return result            def _analyze_gary_dpi_factors(self,
        observation: np.ndarray,        market_context: Optional[Dict[str, Any]]) -> Dict[str, Any]:        \"\"\"Analyze Gary's Dynamic Position Intelligence factors.\"\"\"'
        # Extract relevant features from observation        if len(observation.shape) == 2:  # Sequential data
        latest_features = observation[-1]  # Most recent timestep
        else:                latest_features = observation                    # Gary's key factors (assuming specific feature positions)'        # This would be customized based on actual feature engineering                price_momentum = latest_features[0] if len(latest_features) > 0 else 0                volume_pressure = latest_features[1] if len(latest_features) > 1 else 0                volatility_regime = latest_features[2] if len(latest_features) > 2 else 0                        return {                'price_momentum': float(price_momentum),                'volume_pressure': float(volume_pressure),                'volatility_regime': float(volatility_regime),                'momentum_strength': abs(price_momentum),                'volume_confirmation': volume_pressure > 0.1,                'regime_stability': abs(volatility_regime - 1) < 0.2,                'dpi_composite_score': (abs(price_momentum) + max(0, volume_pressure) +                 (1 - abs(volatility_regime - 1))) / 3                )            def _assess_taleb_antifragility(self,
        observation: np.ndarray,        market_context: Optional[Dict[str, Any]]) -> Dict[str, Any]:        \"\"\"Assess Taleb's antifragility factors.\"\"\"'
        if len(observation.shape) == 2:
                latest_features = observation[-1]        else:
        latest_features = observation                    # Antifragility indicators                    volatility = market_context.get('volatility', 0.2) if market_context else 0.2                    tail_risk = latest_features[3] if len(latest_features) > 3 else -0.02                    upside_capture = latest_features[4] if len(latest_features) > 4 else 0.03                            return {                    'volatility_opportunity': float(volatility),                    'tail_risk': float(tail_risk),                    'upside_capture': float(upside_capture),                    'asymmetric_payoff_ratio': float(abs(upside_capture) / (abs(tail_risk) + 1e-8)),                    'convexity_benefit': float(volatility * 0.5),  # Simplified convexity measure                    'antifragile_score': float((volatility + abs(upside_capture) - abs(tail_risk)) / 3),                    'stress_opportunity': volatility > 0.3,  # High volatility as opportunity                    'black_swan_prepared': abs(tail_risk) < 0.05  # Limited downside                    )            def _adapt_strategy(self, eval_results: Dict[str, Any]):
            pass

        \"\"\"Adapt strategy based on evaluation results.\"\"\"
        avg_return = eval_results['avg_return']
        avg_sharpe = eval_results['avg_sharpe_ratio']
                # Check if performance is below threshold        if (avg_return < self.performance_threshold or
        avg_sharpe < 1.0):
                    # Increase exploration (entropy coefficient)
        if hasattr(self.agent, 'config'):
        self.agent.config.entropy_coef = min(0.02, self.agent.config.entropy_coef * 1.1)                        # Adjust GaryTaleb weights based on their contribution

                gary_contribution = eval_results.get('avg_gary_dpi_score', 0)                taleb_contribution = eval_results.get('avg_antifragile_score', 0)                            if gary_contribution < 0:                    self.config.dpi_reward_weight *= 0.9  # Reduce DPI weight                else:                        self.config.dpi_reward_weight = min(0.5, self.config.dpi_reward_weight * 1.1)                                        if taleb_contribution < 0:                            self.config.antifragile_reward_weight *= 0.9                        else:                                self.config.antifragile_reward_weight = min(0.3, self.config.antifragile_reward_weight * 1.1)                    def _check_performance_targets(self) -> Dict[str, bool]:
        \"\"\"Check if performance targets are met.\"\"\"
        if len(self.training_history['returns']) < 50:
        return {'insufficient_data': True)                            recent_returns = self.training_history['returns'][-50:]                recent_sharpe = self.training_history['sharpe_ratios'][-50:]                recent_drawdowns = self.training_history['max_drawdowns'][-50:]                        return {                'return_target_met': np.mean(recent_returns) >= self.config.target_return,                'drawdown_target_met': np.mean(recent_drawdowns) <= self.config.max_drawdown_limit,                'sharpe_target_met': np.mean(recent_sharpe) >= self.config.min_sharpe_ratio,                'inference_target_met': self.agent.get_performance_metrics().get('inference_target_met', False),                'overall_target_met': (                np.mean(recent_returns) >= self.config.target_return and                np.mean(recent_drawdowns) <= self.config.max_drawdown_limit and                np.mean(recent_sharpe) >= self.config.min_sharpe_ratio                )                )            def save_model(self, path: str):
            pass

        \"\"\"Save complete strategy optimizer state.\"\"\"
        self.agent.save_model(path)
                # Also save strategy-specific state        strategy_state = {
        'config': self.config,
        'training_history': self.training_history,
        'gary_dpi_state': self.gary_dpi_state,
        'taleb_antifragile_state': self.taleb_antifragile_state
        )
                import pickle

        with open(strategy_path, 'wb') as f:
                pickle.dump(strategy_state, f)                def load_model(self, path: str):
        \"\"\"Load complete strategy optimizer state.\"\"\"
        self.agent.load_model(path)
                # Load strategy-specific state        import pickle
        strategy_path = path.replace('.pt', '_strategy.pkl')

        try:
        with open(strategy_path, 'rb'} as f:                    strategy_state = pickle.load(f}                    self.training_history = strategy_state.get('training_history', self.training_history}                    self.gary_dpi_state = strategy_state.get('gary_dpi_state', self.gary_dpi_state}                    self.taleb_antifragile_state = strategy_state.get('taleb_antifragile_state', self.taleb_antifragile_state}                except FileNotFoundError:                        print(\"Strategy state file not found, using defaults\"}                def get_comprehensive_metrics(self} -> Dict[str, Any]:
            pass

        \"\"\"Get comprehensive performance metrics.\"\"\"
        agent_metrics = self.agent.get_performance_metrics(}
        if len(self.training_history['returns']} > 0:
            pass

                training_metrics = {                'total_training_episodes': len(self.training_history['returns']},                'avg_training_return': np.mean(self.training_history['returns']},                'best_training_return': np.max(self.training_history['returns']},                'avg_training_sharpe': np.mean(self.training_history['sharpe_ratios']},                'avg_training_drawdown': np.mean(self.training_history['max_drawdowns']},                'avg_win_rate': np.mean(self.training_history['win_rates']}                }        else:
        training_metrics = {'status': 'No training data available'}                                gary_dpi_metrics = {                    'dpi_enabled': self.config.dpi_reward_weight > 0,                    'dpi_weight': self.config.dpi_reward_weight,                    'avg_dpi_contribution': np.mean(self.training_history['gary_dpi_contributions']} if self.training_history['gary_dpi_contributions'] else 0                    }                            taleb_metrics = {                    'antifragile_enabled': self.config.antifragile_reward_weight > 0,                    'antifragile_weight': self.config.antifragile_reward_weight,                    'avg_antifragile_contribution': np.mean(self.training_history['taleb_antifragile_contributions']} if self.training_history['taleb_antifragile_contributions'] else 0                    }                            performance_targets = self._check_performance_targets(}                            return {                    'agent_metrics': agent_metrics,                    'training_metrics': training_metrics,                    'gary_dpi_metrics': gary_dpi_metrics,                    'taleb_metrics': taleb_metrics,                    'performance_targets': performance_targets,                    'config': self.config,                    'algorithm': self.config.algorithm,                    'ready_for_production': performance_targets.get('overall_target_met', False}                    }# Factory function for easy creation    def create_strategy_optimizer(

        market_data: pd.DataFrame,        initial_capital: float = 200.0,        algorithm: str = 'ppo',        enable_gary_dpi: bool = True,        enable_taleb_antifragile: bool = True,        fast_inference: bool = True        } -> StrategyOptimizerRL:        \"\"\"Create strategy optimizer with GaryTaleb integration.            Args:        market_data: Historical market data
        initial_capital: Starting capital
        algorithm: RL algorithm ('ppo' or 'a3c'}
        enable_gary_dpi: Enable Gary's DPI integration'
        enable_taleb_antifragile: Enable Taleb's antifragility'
        fast_inference: Optimize for fast inference
                Returns:
                Configured strategy optimizer                \"\"\"                config = StrategyConfig(                algorithm=algorithm,                initial_capital=initial_capital,                dpi_reward_weight=0.3 if enable_gary_dpi else 0.0,                antifragile_reward_weight=0.2 if enable_taleb_antifragile else 0.0,                target_inference_ms=50.0 if fast_inference else 100.0                }                    optimizer = StrategyOptimizerRL(market_data, config}                    print(f\"Strategy Optimizer created with {algorithm.upper(}} algorithm\"}                print(f\"Initial capital: ${initial_capital}\"}                print(f\"GaryTaleb integration: DPI={enable_gary_dpi}, Antifragile={enable_taleb_antifragile}\"}                print(f\"Target inference time: {config.target_inference_ms}ms\"}                    return optimizer""
