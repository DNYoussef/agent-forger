"""PPO Agent for Trading Strategy Optimization"Proximal Policy Optimization agent optimized for financial trading.Includes GaryTaleb integration and fast inference capabilities."""import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.distributions import Normalimport numpy as npfrom typing import Dict, List, Tuple, Optional, Anyfrom dataclasses import dataclassimport timefrom collections import dequeimport random@dataclassclass PPOConfig:        """PPO agent configuration."""    # Network architecture        hidden_size: int = 256        num_layers: int = 3        activation: str = 'relu'    # PPO hyperparameters        learning_rate: float = 3e-4        gamma: float = 0.99  # Discount factor        gae_lambda: float = 0.95  # GAE parameter        clip_ratio: float = 0.2  # PPO clipping parameter        value_loss_coef: float = 0.5        entropy_coef: float = 0.01        max_grad_norm: float = 1.0    # Training parameters        batch_size: int = 256        mini_batch_size: int = 64        ppo_epochs: int = 4        buffer_size: int = 10000        update_frequency: int = 2048    # GaryxTaleb integration        dpi_feature_weight: float = 0.3        antifragile_exploration_weight: float = 0.2        volatility_adaptation: bool = True    # Performance optimization        use_lstm: bool = True        compile_model: bool = True        target_inference_ms: float = 50.0class ActorCritic(nn.Module):        """Actor-Critic network for PPO with financial enhancements."""    def __init__(self,
        observation_shape: Tuple[int, ...],        action_dim: int,        config: PPOConfig):            """Initialize Actor-Critic network."        Args:
                observation_shape: Shape of observation space                action_dim: Dimension of action space                config: PPO configuration                """                super(ActorCritic, self).__init__()                self.config = config        # Calculate input size                if len(observation_shape) == 2:  # Sequential data                self.sequence_length, self.feature_dim = observation_shape                self.use_sequential = True        else:
                    self.feature_dim = np.prod(observation_shape)                    self.sequence_length = 1                    self.use_sequential = False        # Feature extraction layers                    if self.use_sequential and config.use_lstm:                        self.feature_extractor = nn.LSTM(                        input_size=self.feature_dim,                        hidden_size=config.hidden_size,                        num_layers=2,                        batch_first=True,                        dropout=0.1                        )                        feature_output_size = config.hidden_size                    else:        # Dense feature extraction
                        pass  # Auto-fixed: empty block                        pass  # Auto-fixed: empty block                        pass  # Auto-fixed: empty block                        pass  # Auto-fixed: empty block                        pass  # Auto-fixed: empty block                        layers = []                        input_size = self.feature_dim if not self.use_sequential else self.sequence_length * self.feature_dim                        for i in range(config.num_layers):                            output_size = config.hidden_size // (2 ** i) if i > 0 else config.hidden_size                            layers.extend([                            nn.Linear(input_size, output_size),                            self._get_activation(config.activation),                            nn.Dropout(0.1)                            ])                            input_size = output_size                            self.feature_extractor = nn.Sequential(*layers)                            feature_output_size = input_size        # Gary's DPI integration layer'                            self.dpi_processor = nn.Sequential(                            nn.Linear(feature_output_size, feature_output_size // 2),                            nn.ReLU(),                            nn.Linear(feature_output_size // 2, feature_output_size),                            nn.Sigmoid()                            )        # Taleb's antifragility enhancement'                            self.antifragile_gate = nn.Sequential(                            nn.Linear(feature_output_size, feature_output_size // 4),                            nn.Tanh(),                            nn.Linear(feature_output_size // 4, feature_output_size),                            nn.Sigmoid()                            )        # Policy head (actor)                            self.actor_mean = nn.Sequential(                            nn.Linear(feature_output_size, config.hidden_size // 2),                            nn.ReLU(),                            nn.Linear(config.hidden_size // 2, action_dim),                            nn.Tanh()  # Actions in [-1, 1]                            )        # Policy standard deviation (learnable)                            self.actor_std = nn.Parameter(torch.ones(action_dim) * 0.5)        # Value head (critic)                            self.critic = nn.Sequential(                            nn.Linear(feature_output_size, config.hidden_size // 2),                            nn.ReLU(),                            nn.Linear(config.hidden_size // 2, config.hidden_size // 4),                            nn.ReLU(),                            nn.Linear(config.hidden_size // 4, 1)                            )        # Volatility adaptation network                            if config.volatility_adaptation:                                self.volatility_adapter = nn.Sequential(                                nn.Linear(1, config.hidden_size // 8),  # Single volatility input                                nn.ReLU(),                                nn.Linear(config.hidden_size // 8, feature_output_size),                                nn.Sigmoid()                                )                            else:                                    self.volatility_adapter = None        # Initialize weights                                    self._init_weights()    def _get_activation(self, activation: str) -> nn.Module:
        \"\"\"Get activation function.\"\"\"        if activation == 'relu':            return nn.ReLU()        elif activation == 'tanh':            return nn.Tanh()        elif activation == 'elu':            return nn.ELU()        else:            return nn.ReLU()                def _init_weights(self):        \"\"\"Initialize network weights.\"\"\"        for module in self.modules():            if isinstance(module, nn.Linear):                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))                nn.init.zeros_(module.bias)            elif isinstance(module, nn.LSTM):                for name, param in module.named_parameters():                    if 'weight' in name:                        nn.init.orthogonal_(param)                    elif 'bias' in name:                        nn.init.zeros_(param)                            def forward(self,                 observations: torch.Tensor,                volatility: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:        \"\"\"Forward pass through Actor-Critic network.                Args:            observations: Input observations            volatility: Optional volatility information                    Returns:            action_mean, action_std, value        \"\"\"        batch_size = observations.shape[0]                # Feature extraction        if self.use_sequential and self.config.use_lstm:            if len(observations.shape) == 3:  # [batch, seq, features]                features, _ = self.feature_extractor(observations)                features = features[:, -1, :]  # Take last output            else:                observations_reshaped = observations.view(batch_size, self.sequence_length, -1)                features, _ = self.feature_extractor(observations_reshaped)                features = features[:, -1, :]        else:            if len(observations.shape) > 2:                observations = observations.view(batch_size, -1)            features = self.feature_extractor(observations)                    # Gary's DPI enhancement\n        if self.config.dpi_feature_weight > 0:\n            dpi_weights = self.dpi_processor(features)\n            features = features * (1 + self.config.dpi_feature_weight * dpi_weights)\n            \n        # Taleb's antifragility enhancement        if self.config.antifragile_exploration_weight > 0:            antifragile_boost = self.antifragile_gate(features)            features = features * (1 + self.config.antifragile_exploration_weight * antifragile_boost)                    # Volatility adaptation        if self.volatility_adapter is not None and volatility is not None:            if len(volatility.shape) == 1:                volatility = volatility.unsqueeze(-1)            vol_weights = self.volatility_adapter(volatility)            features = features * vol_weights                    # Policy (actor) output        action_mean = self.actor_mean(features)        action_std = F.softplus(self.actor_std).expand_as(action_mean)                # Value (critic) output        value = self.critic(features)                return action_mean, action_std, value            def get_action_and_value(self,                            observations: torch.Tensor,                           actions: Optional[torch.Tensor] = None,                           volatility: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:        \"\"\"Get action and value with log probabilities.                Args:            observations: Input observations            actions: Optional actions for evaluation            volatility: Optional volatility information                    Returns:            Dictionary with actions, log_probs, values, and entropy        \"\"\"        action_mean, action_std, values = self.forward(observations, volatility)                # Create normal distribution        dist = Normal(action_mean, action_std)                if actions is None:            # Sample new actions            actions = dist.sample()                    log_probs = dist.log_prob(actions).sum(-1)        entropy = dist.entropy().sum(-1)                return {            'actions': actions,            'log_probs': log_probs,            'values': values.squeeze(-1),            'entropy': entropy,            'action_mean': action_mean,            'action_std': action_std)class PPOAgent:    \"\"\"PPO Agent for trading strategy optimization.        Features:    - Gary's DPI integration\n    - Taleb's antifragility principles    - Volatility-adaptive exploration    - Fast inference optimization    \"\"\"        def __init__(self,                 observation_shape: Tuple[int, ...],                 action_dim: int,                 config: PPOConfig):        \"\"\"Initialize PPO agent.                Args:            observation_shape: Shape of observation space            action_dim: Dimension of action space            config: PPO configuration        \"\"\"        self.config = config        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')                # Create network        self.actor_critic = ActorCritic(observation_shape, action_dim, config).to(self.device)                # Optimizer        self.optimizer = optim.Adam(            self.actor_critic.parameters(),            lr=config.learning_rate,            eps=1e-5        )                # Learning rate scheduler        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(            self.optimizer, mode='max', patience=10, factor=0.8        )                # Experience buffer        self.buffer = PPOBuffer(            observation_shape,            action_dim,            config.buffer_size,            config.gamma,            config.gae_lambda        )                # Performance tracking        self.inference_times = deque(maxlen=1000)        self.training_stats = {            'total_updates': 0,            'policy_losses': [],            'value_losses': [],            'entropy_losses': [],            'returns': [])                # Compile model for faster inference        if config.compile_model and torch.__version__ >= \"2.0.0\":            self.actor_critic = torch.compile(self.actor_critic, mode='max-autotune')                def get_action(self,                    observation: np.ndarray,                   volatility: Optional[float] = None,                   deterministic: bool = False) -> Tuple[np.ndarray, Dict[str, Any]]:        \"\"\"Get action for given observation.                Args:            observation: Current observation            volatility: Current market volatility            deterministic: Whether to use deterministic policy                    Returns:            action, info_dict        \"\"\"        start_time = time.time()                with torch.no_grad():            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(self.device)                        vol_tensor = None            if volatility is not None:                vol_tensor = torch.FloatTensor([volatility]).to(self.device)                            if deterministic:                # Use mean action for deterministic policy                action_mean, _, value = self.actor_critic(obs_tensor, vol_tensor)                action = action_mean                log_prob = torch.zeros(1)                entropy = torch.zeros(1)            else:                # Sample action stochastically                output = self.actor_critic.get_action_and_value(obs_tensor, volatility=vol_tensor)                action = output['actions']                log_prob = output['log_probs']                entropy = output['entropy']                value = output['values']                        # Track inference time        inference_time = (time.time() - start_time) * 1000  # ms        self.inference_times.append(inference_time)                action_np = action.cpu().numpy()[0]                info = {            'value': float(value.cpu().numpy()[0]) if not deterministic else 0.0,            'log_prob': float(log_prob.cpu().numpy()[0]) if not deterministic else 0.0,            'entropy': float(entropy.cpu().numpy()[0]) if not deterministic else 0.0,            'inference_time_ms': inference_time,            'deterministic': deterministic,            'volatility_adapted': volatility is not None)                return action_np, info            def store_transition(self,                       observation: np.ndarray,                       action: np.ndarray,                       reward: float,                       value: float,                       log_prob: float,                       done: bool,                       volatility: Optional[float] = None):        \"\"\"Store transition in buffer.                Args:            observation: Current observation            action: Action taken            reward: Reward received            value: Value estimate            log_prob: Log probability of action            done: Whether episode is done            volatility: Optional volatility information        \"\"\"        self.buffer.store(            observation=observation,            action=action,            reward=reward,            value=value,            log_prob=log_prob,            done=done,            volatility=volatility        )            def update(self) -> Dict[str, float]:        \"\"\"Update policy using PPO.                Returns:            Training statistics        \"\"\"        if len(self.buffer) < self.config.batch_size:            return {}                    # Get batch data        batch = self.buffer.get_batch()                # Convert to tensors        observations = torch.FloatTensor(batch['observations']).to(self.device)        actions = torch.FloatTensor(batch['actions']).to(self.device)        old_log_probs = torch.FloatTensor(batch['log_probs']).to(self.device)        advantages = torch.FloatTensor(batch['advantages']).to(self.device)        returns = torch.FloatTensor(batch['returns']).to(self.device)        volatility = None        if batch['volatility'] is not None:            volatility = torch.FloatTensor(batch['volatility']).to(self.device)                    # Normalize advantages        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)                # PPO updates        policy_losses = []        value_losses = []        entropy_losses = []                # Create mini-batches        batch_size = len(observations)        indices = np.arange(batch_size)                for epoch in range(self.config.ppo_epochs):            np.random.shuffle(indices)                        for start in range(0, batch_size, self.config.mini_batch_size):                end = start + self.config.mini_batch_size                mb_indices = indices[start:end]                                # Mini-batch data                mb_obs = observations[mb_indices]                mb_actions = actions[mb_indices]                mb_old_log_probs = old_log_probs[mb_indices]                mb_advantages = advantages[mb_indices]                mb_returns = returns[mb_indices]                mb_volatility = volatility[mb_indices] if volatility is not None else None                                # Forward pass                output = self.actor_critic.get_action_and_value(                    mb_obs, mb_actions, mb_volatility                )                                new_log_probs = output['log_probs']                values = output['values']                entropy = output['entropy']                                # Policy loss (PPO clipped objective)                ratio = torch.exp(new_log_probs - mb_old_log_probs)                surr1 = ratio * mb_advantages                surr2 = torch.clamp(                    ratio,                     1 - self.config.clip_ratio,                    1 + self.config.clip_ratio                ) * mb_advantages                                policy_loss = -torch.min(surr1, surr2).mean()                                # Value loss                value_loss = F.mse_loss(values, mb_returns)                                # Entropy loss (for exploration)                entropy_loss = -entropy.mean()                                # Total loss                total_loss = (                    policy_loss +                     self.config.value_loss_coef * value_loss +                    self.config.entropy_coef * entropy_loss                )                                # Backward pass                self.optimizer.zero_grad()                total_loss.backward()                                # Gradient clipping                torch.nn.utils.clip_grad_norm_(                    self.actor_critic.parameters(),                    self.config.max_grad_norm                )                                self.optimizer.step()                                # Store losses                policy_losses.append(policy_loss.item())                value_losses.append(value_loss.item())                entropy_losses.append(entropy_loss.item())                        # Update statistics        self.training_stats['total_updates'] += 1        self.training_stats['policy_losses'].extend(policy_losses)        self.training_stats['value_losses'].extend(value_losses)        self.training_stats['entropy_losses'].extend(entropy_losses)        self.training_stats['returns'].extend(returns.cpu().numpy().tolist())                # Learning rate scheduling        avg_return = np.mean(returns.cpu().numpy())        self.scheduler.step(avg_return)                # Clear buffer        self.buffer.clear()                return {            'policy_loss': np.mean(policy_losses),            'value_loss': np.mean(value_losses),            'entropy_loss': np.mean(entropy_losses),            'avg_return': avg_return,            'learning_rate': self.optimizer.param_groups[0]['lr'])            def save_model(self, path: str):        \"\"\"Save model checkpoint.                Args:            path: Save path        \"\"\"        torch.save({            'actor_critic_state_dict': self.actor_critic.state_dict(),            'optimizer_state_dict': self.optimizer.state_dict(),            'config': self.config,            'training_stats': self.training_stats), path)            def load_model(self, path: str):        \"\"\"Load model checkpoint.                Args:            path: Load path        \"\"\"        checkpoint = torch.load(path, map_location=self.device)        self.actor_critic.load_state_dict(checkpoint['actor_critic_state_dict'])        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])        self.training_stats = checkpoint.get('training_stats', self.training_stats)            def get_performance_metrics(self) -> Dict[str, Any]:        \"\"\"Get agent performance metrics.\"\"\"        if not self.inference_times:            return {'status': 'No inference data available'}                    recent_times = list(self.inference_times)[-100:]                metrics = {            'avg_inference_time_ms': np.mean(recent_times),            'max_inference_time_ms': np.max(recent_times),            'min_inference_time_ms': np.min(recent_times),            'inference_target_met': np.mean(recent_times) < self.config.target_inference_ms,            'total_updates': self.training_stats['total_updates'],            'avg_policy_loss': np.mean(self.training_stats['policy_losses'][-100:]) if self.training_stats['policy_losses'] else 0,            'avg_value_loss': np.mean(self.training_stats['value_losses'][-100:]) if self.training_stats['value_losses'] else 0,            'avg_return': np.mean(self.training_stats['returns'][-100:]) if self.training_stats['returns'] else 0,            'current_lr': self.optimizer.param_groups[0]['lr'],            'model_parameters': sum(p.numel() for p in self.actor_critic.parameters()),            'gary_dpi_enabled': self.config.dpi_feature_weight > 0,            'taleb_antifragile_enabled': self.config.antifragile_exploration_weight > 0,            'volatility_adaptation': self.config.volatility_adaptation)                return metricsclass PPOBuffer:    \"\"\"Experience buffer for PPO.\"\"\"        def __init__(self,                 observation_shape: Tuple[int, ...],                 action_dim: int,                 buffer_size: int,                 gamma: float,                 gae_lambda: float):        \"\"\"Initialize buffer.                Args:            observation_shape: Shape of observations            action_dim: Dimension of actions            buffer_size: Maximum buffer size            gamma: Discount factor            gae_lambda: GAE parameter        \"\"\"        self.observation_shape = observation_shape        self.action_dim = action_dim        self.buffer_size = buffer_size        self.gamma = gamma        self.gae_lambda = gae_lambda                # Storage arrays        self.observations = np.zeros((buffer_size,) + observation_shape, dtype=np.float32)        self.actions = np.zeros((buffer_size, action_dim), dtype=np.float32)        self.rewards = np.zeros(buffer_size, dtype=np.float32)        self.values = np.zeros(buffer_size, dtype=np.float32)        self.log_probs = np.zeros(buffer_size, dtype=np.float32)        self.dones = np.zeros(buffer_size, dtype=np.bool_)        self.volatility = np.zeros(buffer_size, dtype=np.float32)                self.ptr = 0        self.size = 0            def store(self,             observation: np.ndarray,             action: np.ndarray,             reward: float,             value: float,             log_prob: float,             done: bool,             volatility: Optional[float] = None):        \"\"\"Store transition.\"\"\"        idx = self.ptr % self.buffer_size                self.observations[idx] = observation        self.actions[idx] = action        self.rewards[idx] = reward        self.values[idx] = value        self.log_probs[idx] = log_prob        self.dones[idx] = done        self.volatility[idx] = volatility if volatility is not None else 0.0                self.ptr += 1        self.size = min(self.size + 1, self.buffer_size)            def get_batch(self) -> Dict[str, np.ndarray]:        \"\"\"Get batch with advantages and returns calculated.\"\"\"        assert self.size >= self.buffer_size, \"Buffer not full\"                # Calculate advantages and returns using GAE        advantages = np.zeros_like(self.rewards)        returns = np.zeros_like(self.rewards)                gae = 0        for t in reversed(range(self.buffer_size)):            if t == self.buffer_size - 1:                next_value = 0  # Assuming episode ends            else:                next_value = self.values[t + 1]                            delta = self.rewards[t] + self.gamma * next_value - self.values[t]            gae = delta + self.gamma * self.gae_lambda * gae            advantages[t] = gae            returns[t] = gae + self.values[t]                    return {            'observations': self.observations[:self.size].copy(),            'actions': self.actions[:self.size].copy(),            'log_probs': self.log_probs[:self.size].copy(),            'advantages': advantages[:self.size].copy(),            'returns': returns[:self.size].copy(),            'volatility': self.volatility[:self.size].copy() if np.any(self.volatility) else None)            def clear(self):        \"\"\"Clear buffer.\"\"\"        self.ptr = 0        self.size = 0            def __len__(self) -> int:        return self.size# Factory function for easy creationdef create_ppo_agent(    observation_shape: Tuple[int, ...],    action_dim: int = 2,  # [position_change, confidence]    learning_rate: float = 3e-4,    enable_dpi: bool = True,    enable_antifragile: bool = True,    fast_inference: bool = True) -> PPOAgent:    \"\"\"Create PPO agent with GaryTaleb integration.        Args:        observation_shape: Shape of observation space        action_dim: Dimension of action space        learning_rate: Learning rate        enable_dpi: Enable Gary's DPI features\n        enable_antifragile: Enable Taleb's antifragility        fast_inference: Optimize for fast inference            Returns:        Configured PPO agent    \"\"\"    config = PPOConfig(        learning_rate=learning_rate,        dpi_feature_weight=0.3 if enable_dpi else 0.0,        antifragile_exploration_weight=0.2 if enable_antifragile else 0.0,        volatility_adaptation=True,        compile_model=fast_inference,        target_inference_ms=50.0 if fast_inference else 100.0    )        agent = PPOAgent(observation_shape, action_dim, config)        print(f\"PPO Agent created for trading strategy optimization\")    print(f\"GaryTaleb integration: DPI={enable_dpi), Antifragile={enable_antifragile)\")    print(f\"Target inference time: {config.target_inference_ms)ms\")        return agent""}}}}}}}}}}
